---
title: "Understanding State Preferences With Text As Data: the 2018 update"
author:
- affiliation: Dublin City University
  email: alex.baturo@dcu.ie
  name: Alexander Baturo
- affiliation: University of Birmingham
  email: n.dasandi@bham.ac.uk
  name: Niheer Dasandi
- affiliation: University of Essex
  email: s.mikhaylov@essex.ac.uk
  name: Slava Jankin Mikhaylov
date: 21 August 2018
output:
  html_notebook:
    toc: yes
  html_document: default
  pdf_document: default
  toc: yes
  word_document: default
#biblio-style: apsr
#bibliography: un.bib

abstract: This update addresses two things -- changes to `quanteda` API that broke some code and additional data available that now covers the period 1970-2017. 
---


##Reading in UNGDC

```{r}
#Loading packages and data
library(readtext)
library(quanteda)
library(dplyr)
library(stringr)
library(ggplot2)
library(rworldmap)
library(RColorBrewer)
library(classInt)
library(readxl)
library(ggridges)
library(viridis)
```

Zip archive with the UNGDC data will need to be downloaded and file path below pointed to the folder.

```{r}

#File path to UNGDC unzipped data archive
DATA_DIR <- "~/Dropbox/Research/UN Data/" 

#Reading in text files
ungd_files <- readtext(paste0(DATA_DIR, "Converted sessions/*"), 
                                 docvarsfrom = "filenames", 
                                 dvsep="_", 
                                 docvarnames = c("Country", "Session", "Year"))

#changing row.names to have only country_year, rather than folder pathway from `readtext`.

 ungd_files$doc_id <- str_replace(ungd_files$doc_id , ".txt", "") %>%
   str_replace(. , "_\\d{2}", "")
 
```


##Creating corpus

```{r}
ungd_corpus <- corpus(ungd_files, text_field = "text") 

```



##UNGDC corpus summary

```{r}

summarise(group_by(summary(ungd_corpus, n = 7897),Year),
                           mean(Types),mean(Tokens),
                           mean(Sentences),min(Sentences),max(Sentences))

```

##Summary of speakers

As discussed in the text we categorize speakers by:

1) heads of state or government (e.g. presidents, prime ministers, kings); 
2) vice-presidents, deputy prime ministers, and foreign ministers; 
3) country representative at the UN.

```{r}
#Reading in Excel spreadsheet with speakers information
posts <- read_excel("posts_of_speakers.xlsx")
table <- table(posts$Post)
table
```


```{r}
#Proportions by category
prop.table(table)
```


##Tokenizing documents

```{r}
#Tokenization and basic pre-processing
tok <- tokens(ungd_corpus, what = "word",
              remove_punct = TRUE,
              remove_symbols = TRUE,
              remove_numbers = TRUE,
              remove_twitter = TRUE,
              remove_url = TRUE,
              remove_hyphens = TRUE,
              verbose = TRUE, 
              include_docvars = TRUE)
```

##Removing some features

As part of converting original transcripts held by UN library to digital, machine-readable format, we had to OCR documents before 1994. That introduced several tokens that were compounded with digits, punctuation and other OCR errors. We are cleaning it up here with brute force purging of tokens.

```{r}
tok.m <- tokens_select(tok, c("[\\d-]", "[[:punct:]]", "^.{1,2}$"), 
                       selection = "remove", 
                    valuetype="regex", verbose = TRUE)

tok.r <- tokens_tolower(tok.m)
```


##Creating document feature matrix


```{r dfm}
#DFM creation from tokens, removing stopwords, and stemming.
dfm <- dfm(tok.r, 
           tolower = TRUE,
           remove=stopwords("english"),
           stem=TRUE, 
           verbose = TRUE)

#Showing 100 most frequrent tokens in DFM
topfeatures(dfm, n = 100, decreasing = FALSE)

```




##TF-IDF weighting

Some of the least frequent terms above are due to OCR and text digitization errors. However, rather than dropping the terms arbitrarily we implement TF-IDF weighting.

```{r}
dfm.w <- dfm_tfidf(dfm)
```



##Wordscore estimates

Estimates are from 1971 as Russia hasn't made a UNGD statement in 1970. We are estimating year by year separately as one albeit imperfect way to address concept drift.

```{r}
#Wordscore estimations by year
rusa <- data.frame()
for (i in 1971:2017) {

#Creating corpus for each year
ungdc.i <- corpus_subset(ungd_corpus, Year==i)

tok <- tokens(ungdc.i, what = "word",
              remove_punct = TRUE,
              remove_symbols = TRUE,
              remove_numbers = TRUE,
              remove_twitter = TRUE,
              remove_url = TRUE,
              remove_hyphens = TRUE,
              verbose = TRUE)

tok.m <- tokens_select(tok, c("[\\d-]", "[[:punct:]]", "^.{1,2}$"), 
                       selection = "remove", 
                    valuetype="regex", verbose = TRUE)

tok.r <- tokens_tolower(tok.m)

dfm <- dfm(tok.r, 
           tolower = TRUE,
           remove=stopwords("english"),
           stem=TRUE, 
           verbose = TRUE)


#tfidf weighting
dfm.w <- dfm_tfidf(dfm)

#Reference scores
refscores <- rep(NA,nrow(dfm.w))

refscores[dfm.w@docvars$Country=="RUS"] <- -1
refscores[dfm.w@docvars$Country=="USA"] <- 1



#Wordscore model
ws <- textmodel_wordscores(dfm.w, refscores, scale="linear", smooth=1)
wordscore <- predict(ws, rescaling="none")

#Writing the results into data frame
wordscores.i <- data.frame(cbind(docvars(ungdc.i), wordscore))

rusa <- rbind(rusa,wordscores.i)

}
```



##Map with 2014 wordscore results

```{r}
sPDF <- joinCountryData2Map(subset(rusa, Year==2014),
                            joinCode="ISO3",nameJoinColumn="Country")

new_world <- subset(sPDF, continent != "Antarctica")


#Setting up class intervals for continuous variable
classInt <- classIntervals(sPDF$wordscore,
                           style="kmeans")

catMethod=classInt$brks

#Selecting diverging palette
colourPalette <- brewer.pal(9,"Blues")

#Drawing the map

mapParams <- mapCountryData(new_world,nameColumnToPlot="wordscore", 
                            catMethod=catMethod,
                            mapTitle="USA vs Russia: Wordscore 2014",
                            colourPalette=colourPalette,
                            missingCountryCol="grey", 
                            addLegend="FALSE")

#adding legend
do.call( addMapLegend, c( mapParams, legendLabels="limits", 
                          labelFontSize=0.7,legendShrink=0.7,
                          legendMar=4, legendWidth=0.6))


```


## France and Germany on the RUS-USA dimension

```{r}
ggplot(data=subset(rusa,Country=="FRA"| Country == "DEU"), 
            aes(x=Year, y=wordscore, group=Country, colour=Country)) +
  theme_bw() +
  ggtitle("Germany and France on RUS-USA dimension") + ylab("Wordscore")+
  theme(plot.title = element_text(lineheight=.8, face="bold", size=15),
        axis.title.x = element_text(face="bold", size=11),
        axis.title.y = element_text(face="bold", size=11),
        axis.text.x  = element_text(angle=90, vjust=0.5, size=8)) +
  scale_y_continuous() +
  scale_x_continuous(breaks = c(1970, 1980, 1990, 2000, 2010, 2017)) +
  geom_smooth(se = FALSE) + 
  geom_point(aes(shape=Country))

```

## Changes in position density over time

```{r}
ggplot(rusa, aes(x=wordscore, y=Year, group = Year,  fill = ..x..)) + 
  geom_density_ridges_gradient(rel_min_height = 0.01, scale = 10) +
  scale_fill_viridis(name = "Wordscore", option = "C", alpha = .6) +
  theme_bw() + xlim(-0.15,0.15) + xlab("Wordscores")  +
  scale_y_continuous(trans = "reverse", breaks = c(1970, 1980, 1990, 2000, 2010, 2017)) +
  ggtitle("Changes in positions over time")  

```


